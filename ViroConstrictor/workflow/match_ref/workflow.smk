import logging
import os
import pprint
import sys

import AminoExtract
import numpy as np
import pandas as pd
import yaml
from pathlib import Path
from Bio import SeqIO, SeqRecord
from snakemake.utils import Paramspace, min_version
from snakemake_interface_executor_plugins.settings import DeploymentMethod

import ViroConstrictor
from ViroConstrictor.workflow.helpers.containers import get_hash
from ViroConstrictor.workflow.helpers.directories import *
from ViroConstrictor.workflow.helpers.generic_workflow_methods import (
    read_fasta,
    segmented_ref_groups,
)
from ViroConstrictor.workflow.helpers.presets import get_preset_parameter

min_version("9.5")
# Elevate the log level of all output generated by the snakemake.logging module to CRITICAL in order to suppress it when snakemake is calling itself in a downstream process.
if "--snakefile" in sys.argv:
    logging.getLogger("snakemake.logging").setLevel(logging.CRITICAL)
VC_STAGE = "MATCHREF"

SAMPLES = {}
with open(config["sample_sheet"]) as sample_sheet_file:
    SAMPLES = yaml.safe_load(sample_sheet_file)

container_base_path = workflow.deployment_settings.apptainer_prefix if not None else ""

samples_df = pd.DataFrame(SAMPLES).transpose().reset_index().rename(columns=dict(index="sample", VIRUS="Virus"))
samples_df = segmented_ref_groups(samples_df)
samples_df = samples_df.explode("segment")
p_space = Paramspace(samples_df[["Virus", "segment", "sample"]], filename_params=["sample"])
wc_folder = "/".join(p_space.wildcard_pattern.split("/")[:-1]) + "/"

# These memory functions are tested in tests/unit/test_dynamic_memory.py
# However, because this is a snakefile instead of a python file, they cannot be imported
# So when these functions are changed, please make sure to also change them in the tests.
def low_memory_job(wildcards, threads, attempt):
    if config["computing_execution"] == "local":
        return min(attempt * threads * 1 * 1000, config["max_local_mem"])
    return attempt * threads * 1 * 1000


def medium_memory_job(wildcards, threads, attempt):
    if config["computing_execution"] == "local":
        return min(attempt * threads * 2 * 1000, config["max_local_mem"])
    return attempt * threads * 2 * 1000


def high_memory_job(wildcards, threads, attempt):
    if config["computing_execution"] == "local":
        return min(attempt * threads * 4 * 1000, config["max_local_mem"])
    return attempt * threads * 4 * 1000
    
def low_runtime_job(wildcards, attempt):
    return attempt * 2

def medium_runtime_job(wildcards, attempt):
    return attempt * 10

def high_runtime_job(wildcards, attempt):
    return attempt * 30

def workflow_script_path(relative_path):
    basepath = workflow.basedir
    return os.path.join(basepath, relative_path)


def workflow_environment_path(filename):
    basepath = os.path.dirname(workflow.basedir)  # moves up one directory from the workflow.basedir
    return os.path.join(basepath, conda_envs, filename)


wildcard_constraints:
    # regular expression to match only alphanumeric characters, underscores, dashes, and dots. exclude '/' and only match the first part of the string.
    segment=r"[\w\-\.\d]+",
    Virus=r"[\w\-\.\d]+",
    sample=r"[\w\-\.\d]+",


localrules:
    all,


rule all:
    input:
        f"{datadir}" "match_ref_results.pkl",
        expand(
            f"{datadir}{matchref}" "{sample}_primers.bed",
            sample=p_space.dataframe["sample"],
        ),
        expand(
            f"{datadir}{matchref}" "{sample}_refs.fasta",
            sample=p_space.dataframe["sample"],
        ),
        expand(
            f"{datadir}{matchref}" "{sample}_feats.gff",
            sample=p_space.dataframe["sample"],
        ),


# preparatory steps
include: workflow.source_path("components/preparation.references.smk")
# selection steps
include: workflow.source_path("components/selection.alignment.smk")
include: workflow.source_path("components/selection.reference.smk")
# filtering steps
include: workflow.source_path("components/filter.features.smk")
include: workflow.source_path("components/filter.primers.smk")


# singular end rule that concatenates all the results into a single file
rule concat_frames:
    input:
        set(
            expand(
                f"{datadir}{matchref}" "{sample}_step2.csv",
                sample=p_space.dataframe["sample"],
            )
        ),
    output:
        f"{datadir}" "match_ref_results.pkl",
    threads: 1
    resources:
        mem_mb=low_memory_job,
        runtime=medium_runtime_job,
    shell:
        """
        python -c \"import pandas as pd; import sys; df = pd.concat([pd.read_csv(f, keep_default_na=False) for f in sys.argv[1:-1]]); df.to_pickle(sys.argv[-1])\" {input} {output}
        """


onsuccess:
    logging.info(f"{'='*20} [green] Finished Match-reference process [/green] {'='*20}")
    logging.info("[green]Finalizing the results and continuing with the main analysis workflow[/green]")
    return True


onerror:
    logging.error("[bold red]An error occurred during the ViroConstrictor match-reference process.[/bold red]")
    logging.error("[bold red]Shutting down... Please check all the inputs and logfiles for any abnormalities and try again.[/bold red]")
    return False
